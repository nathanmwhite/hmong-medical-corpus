{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a SQL database for corpus development and management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpora are useful tools both for analyzing human language and for NLP application development. However, finding a good platform for building a corpus is not always straightforward. Using the `sqlite3` package to create a SQL database to manage our corpus data is an excellent solution, as it provides a means both to maintain the internal structure of the data and to quickly traverse that internal structure.\n",
    "\n",
    "Let's begin by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the database.\n",
    "\n",
    "For a part-of-speech tagged database, we need to have the following tables:\n",
    "1. Documents—to keep track of the original document files\n",
    "2. Part of speech—to keep track of all of the possible parts of speech\n",
    "3. Types—to keep track of all attested word types (or lemmas), rather than the tokens and their varying forms\n",
    "4. Tokens—to keep track of the individual word tokens in each document, as they appear in the original\n",
    "\n",
    "For Hmong in particular, because the language's orthography places spaces between syllables, we need to keep track of which position in the word each type/token represents. As a result, we need a fifth table:\n",
    "5. Word position\n",
    "\n",
    "Languages with more complicated morphology may need additional tables to keep track of the various morphological categories for a given word. Hmong, however, maximally allows only one affix per word plus reduplication, and morpheme boundaries coincide with syllable boundaries—and thus spaces—and so each morpheme is already stored as a type.\n",
    "\n",
    "We do, however, want to encode a category only once in the database, and have references made to it, given proper database structure represented by each normal form (https://www.guru99.com/database-normalization.html). So, we refer to categories in one table using indices in another. For example, to reference parts of speech for each word type, we use the index from the parts of speech table to indicate the part of speech for a given type in the types table.\n",
    "\n",
    "Below, we use `sqlite3.Connection(<database_filename>).cursor().execute` with SQL `CREATE TABLE` commands to create each of the five tables, complete with index references within each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7f0d200eda40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(os.path.expanduser('~/python_workspace/medical_corpus_scripting'))\n",
    "\n",
    "# creates new database\n",
    "conn = sqlite3.Connection('hmcorpus.db')\n",
    "\n",
    "# get cursor\n",
    "crsr = conn.cursor()\n",
    "\n",
    "# string lines to initialize each table in database\n",
    "create_documents = \"\"\"CREATE TABLE documents (\n",
    "index INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT,\n",
    "document_title VARCHAR(50),\n",
    "document_addr VARCHAR(150));\"\"\"\n",
    "\n",
    "create_part_of_speech = \"\"\"CREATE TABLE part_of_speech (\n",
    "index INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT,\n",
    "pos_label VARCHAR(2));\"\"\"\n",
    "\n",
    "create_word_location = \"\"\"CREATE TABLE word_location (\n",
    "index INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT,\n",
    "location CHAR);\"\"\"\n",
    "\n",
    "create_word_types = \"\"\"CREATE TABLE word_types (\n",
    "index INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT,\n",
    "word_type_form VARCHAR(20),\n",
    "word_location INTEGER,\n",
    "pos_type INTEGER,\n",
    "FOREIGN KEY (word_location)\n",
    "REFERENCES word_location(index),\n",
    "FOREIGN KEY (pos_type)\n",
    "REFERENCES part_of_speech(index));\"\"\"\n",
    "\n",
    "create_word_tokens = \"\"\"CREATE TABLE word_tokens (\n",
    "index INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT,\n",
    "document_index INTEGER,\n",
    "sentence_index INTEGER,\n",
    "word_index INTEGER,\n",
    "word_type_index INTEGER,\n",
    "word_token_form VARCHAR(20),\n",
    "FOREIGN KEY (document_index)\n",
    "REFERENCES documents(index),\n",
    "FOREIGN KEY (word_type_index)\n",
    "REFERENCES word_types(index));\"\"\"\n",
    "\n",
    "crsr.execute(create_documents)\n",
    "crsr.execute(create_part_of_speech)\n",
    "crsr.execute(create_word_location)\n",
    "crsr.execute(create_word_types)\n",
    "crsr.execute(create_word_tokens)\n",
    "\n",
    "# set up word_location IOB tags\n",
    "crsr.execute(\"INSERT INTO word_location(location) VALUES ('B'), ('I'), ('O');\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the first file to insert.\n",
    "\n",
    "Next, we use `pickle` to load a file that we want to insert into the database. `pickle` is a module that enables a file to loaded after being handled by another Python script elsewhere. Here, I use it to load a file with contents that have been preprocessed for insertion into the database. Note that this preprocessing step will be the subject of a later blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.expanduser('~/python_workspace/medical_corpus_scripting/pickling'))\n",
    "pickle_file_name = '9_txt.pkl'\n",
    "f = open(pickle_file_name, 'rb')\n",
    "doc_data = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inserting the document information.\n",
    "\n",
    "The preprocessed data contains the text of the document, but not its name or original location. We insert them here using the SQL command `INSERT INTO documents` with the name of the file and its original location inserted from a tuple named `document`. We then run `cursor().execute` to run the SQL command, and use `lastrowid` to retrieve the number the database has assigned our newest document, so that we can use it in insertions when we begin inserting tokens from the file into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = ('Tus Mob Acute Flaccid Myelitis', 'https://www.dhs.wisconsin.gov/publications/p01298h.pdf')\n",
    "insert_doc = \"INSERT INTO docs (doc_title, doc_addr) VALUES ('\" + document[0] + \"', '\" + document[1] + \"');\"\n",
    "doc_index = crsr.execute(insert_doc).lastrowid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function to process each word.\n",
    "\n",
    "Because each document contains hundreds of texts, it is incredibly inefficient to execute a new set of SQL commands for each insertion. As a result, we create a function named `insert_word` below to run each time we insert a word. The function has four parameters: \n",
    "1. `word_tuple`—contains a tuple with the token string and a combined word position/POS tag\n",
    "2. `doc_index_value`—indicates the ID number for the document in the `documents` table\n",
    "3. `sent_index_value`—represents the position in sequence of the current sentence in the document\n",
    "4. `word_index_value`—represents the position in sequence of the current word in the current sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_word(word_tuple, doc_index_value, sent_index_value, word_index_value):\n",
    "    '''\n",
    "    Inserts a word into the database, based on the word_tuple.\n",
    "    @param word_tuple is 3-tuple containing the token's form, the location within a word, and the part of speech\n",
    "    @param doc_index_value is the index of the document from which the word is extracted\n",
    "    @param sent_index_value is the index of the sentence in the document from which the word is extracted\n",
    "    @param word_index_value is the index of the position of the word within its sentence\n",
    "    '''\n",
    "    \n",
    "    # retrieve pos value if found, otherwise add pos value\n",
    "    pos_results = crsr.execute(\"SELECT index FROM part_of_speech WHERE pos_label='\" + word_tuple[2] + \"';\").fetchall()\n",
    "    if len(pos_results) > 0:\n",
    "        pos_label_index = pos_results[0][0]\n",
    "    else:\n",
    "        pos_label_index = crsr.execute(\"INSERT INTO part_of_speech (pos_label) VALUES ('\" + word_tuple[2] + \"');\").lastrowid\n",
    "    \n",
    "    # retrieve relevant word_loc value\n",
    "    if word_tuple[1] in ['B', 'I', 'O']:\n",
    "        word_loc_index = crsr.execute(\"SELECT index FROM word_location WHERE location='\" + word_tuple[1] + \"';\").fetchone()[0]\n",
    "    else:\n",
    "        raise ValueError('Word location value is invalid at word (' + str(sent_index_value - 1) + ', ' \\\n",
    "                        + str(word_index_value - 1) + ').')\n",
    "    \n",
    "    # match word[0].lower(), word_loc_index, pos_label_index against types, and if a match, retrieve index\n",
    "    # if not, add and get index\n",
    "    type_ = word[0].lower()\n",
    "    type_results = crsr.execute(\"SELECT index FROM word_types WHERE word_type_form='\" + type_ + \"' AND word_location=\" \\\n",
    "                                + str(word_loc_index) + \" AND pos_type=\" + str(pos_label_index) + \";\").fetchall()\n",
    "    if len(type_results) > 0:\n",
    "        type_index = type_results[0][0]\n",
    "    else:\n",
    "        type_index = crsr.execute(\"INSERT INTO word_types (word_type_form, word_location, pos_type) VALUES ('\" + type_ + \"', \" \\\n",
    "                                  + str(word_loc_index) + \", \" + str(pos_label_index) + \");\").lastrowid\n",
    "        \n",
    "    # insert complete values into tokens\n",
    "    insertion = crsr.execute(\"INSERT INTO word_tokens (document_index, sentence_index, word_index, word_type_index, word_token_form)\" \\\n",
    "                            + \" VALUES (\" + str(doc_index_value) + \", \" + str(sent_index_value) + \", \" \\\n",
    "                            + str(word_index_value) + \", \" + str(type_index) + \", '\" + word[0] + \"');\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add each token to the database.\n",
    "\n",
    "The next step cycles through the tokens in the file opened with `pickle` above and runs `insert_word` to insert each token in the database. We then close the database, as once we have run this step, we have finished inserting our first document into the database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent in enumerate(doc_data):\n",
    "    for j, word in enumerate(sent):\n",
    "        current_word = tuple([word[0]] + word[1].split('-'))\n",
    "        insert_word(current_word, doc_index, i + 1, j + 1)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We can create a SQL database using the `sqlite3` package to store our data for our corpus. Above, we saw how to create the tables for the corpus using SQL queries and insert our first document. In later posts, we will look at the preprocessing step to convert the original PDF into data ready to insert into the database, and how to use the database to access and search our data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
