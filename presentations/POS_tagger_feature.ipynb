{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A semi-supervised combined word tokenizer-POS tagger for Hmong\n",
    "\n",
    "This post introduces a semi-supervised approach to word tokenization and POS tagging that enables support for resource-poor languages.\n",
    "\n",
    "The Hmong language is a resource-poor language [1] where corpora of POS-tagged data are previously unavailable, precluding supervised approaches. At the same time, the Hmong language has an unusually high number of homonyms and features syllable-based spacing in its orthography, meaning that widespread ambiguity will create serious problems for unsupervised approaches. A semi-supervised approach is in order.\n",
    "\n",
    "The approach featured here follows a relatively unusual strategy: combining word tokenization and POS tagging as a single step. Because Hmong has an orthography where spaces are placed between _syllables_ rather than words, word tokenization will be potentially non-trivial. However, a much more prominent language, Vietnamese, has the same issue, yet unlike Hmong, it is a relatively resource-rich language. This means that, with the relevant adaptations to handle a resource-poor language, approaches that work with Vietnamese should prove useful. One of these approaches is in fact combining word tokenization and POS tagging [2][3].\n",
    "\n",
    "In this approach, word tokenization is combined with POS tagging as a sequence-labeling task where position in the word is handled using IOB tags, where B marks the first syllable of the word, I marks all other syllables of the word, and O marks everything that is not a word. Here, I combine these with POS tags using a hyphen, so that the first syllable of a noun is B-NN and the second syllable of a verb is I-VV.\n",
    "\n",
    "In my approach here, I use pretrained word embeddings. Though Hmong is a resource-poor language, the Internet has proven popular with Hmong speakers, meaning that speakers have produced thousands of forum posts on the soc.culture.hmong listserv over the past 20 years or so. These have been organized into the approximately 12-million token SCH corpus, which is available for free download here: http://listserv.linguistlist.org/pipermail/my-hm/2015-May/000028.html.\n",
    "\n",
    "These pretrained word embeddings are created through Word2Vec and loaded as an embedding layer into a Keras-based BiLSTM model. The BiLSTM model is excellent for the word tokenization/POS tagging task as it is specially designed for handling sequences where individual output values are dependent neighboring values.\n",
    "\n",
    "The model is trained on a set of eight documents—approximately 6000 (actual) words—fully tagged with the combined word position-POS tags mentioned above.\n",
    "\n",
    "Let's begin by importing the relevant libraries.\n",
    "\n",
    "#### Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "from itertools import groupby\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Dense, InputLayer, Embedding, TimeDistributed, Activation\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load existing database with POS-tagged words.\n",
    "\n",
    "Next, we navigate to the folder containing the database file and load the database using sqlite3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.expanduser('~/python_workspace/medical_corpus_scripting/corpus/hminterface/static/hminterface'))\n",
    "conn = sqlite3.Connection('hmcorpus.db')\n",
    "crsr = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve tags from database.\n",
    "\n",
    "Next we retrieve all of the tag types from the database using SQL and creating a dictionary that converts all of the tags to indices that can be used in the Keras model. The result is a unique index for each combination of word position IOB tag and POS tag that are actually attested in the corpus database to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('O-PAD', 0), ('B-CL', 1), ('B-NN', 2), ('O-PU', 3), ('B-FW', 4), ('B-VV', 5), ('B-PP', 6), ('I-NN', 7), ('B-QU', 8), ('I-CL', 9), ('B-LC', 10), ('I-VV', 11), ('B-AD', 12), ('B-DT', 13), ('B-CC', 14), ('I-CC', 15), ('B-CV', 16), ('I-AD', 17), ('B-RL', 18), ('B-CS', 19), ('B-PN', 20), ('I-CS', 21), ('I-FW', 22), ('B-NR', 23), ('I-NR', 24), ('I-PU', 25), ('B-PU', 26), ('B-CM', 27), ('B-ON', 28), ('I-QU', 29), ('I-PN', 30), ('B-JJ', 31)])\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"SELECT DISTINCT loc, pos_label FROM types\n",
    "JOIN word_loc ON word_loc.ind=types.word_loc\n",
    "JOIN pos ON pos.ind=types.pos_type;\"\"\"\n",
    "\n",
    "# set the padding tag combination first, then add tag combinations from database\n",
    "tag_combinations = [('O', 'PAD')]\n",
    "tag_combinations += crsr.execute(query).fetchall()\n",
    "\n",
    "tag_indices = {'-'.join(t): i for i, t in enumerate(tag_combinations)}\n",
    "print(tag_indices.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve word tokens and tags as numerical codes.\n",
    "\n",
    "The database is organized such that each \"word\" (i.e., syllable or punctuation demarcated by spaces) type is assigned its own index in the table `types`. This means that a dataframe can be created using the database data to convert between indices and word types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Word_Type\n",
      "Index             \n",
      "1              tus\n",
      "2              mob\n",
      "3                –\n",
      "4      shigellosis\n",
      "5          disease\n",
      "6             fact\n",
      "7            sheet\n",
      "8           series\n",
      "9              zoo\n",
      "10              li\n",
      "11             cas\n",
      "12               ?\n",
      "13             yog\n",
      "14              ib\n",
      "15             tug\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"SELECT ind, type_form FROM types;\"\"\"\n",
    "word_index_list = crsr.execute(query).fetchall()\n",
    "\n",
    "# Visualize data\n",
    "index_words = DataFrame(data=word_index_list, columns=['Index', 'Word_Type'])\n",
    "index_words.set_index('Index', inplace=True)\n",
    "print(index_words.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following retrieves the word indices from the eight documents stored in the corpus database that we are going to use, and uses the `itertools.groupby` function to organize them in sequence as a list of sentence lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4,), (13,), (14,), (15,), (2,), (16,), (17,), (18,), (19,), (20,), (21,), (16,), (22,)]\n",
      "['shigellosis', 'yog', 'ib', 'tug', 'mob', 'los', 'ntawm', 'cov', 'kab', 'mob', 'bacteria', 'los', '.']\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"SELECT doc_ind, sent_ind, word_type_ind, loc, pos_label FROM tokens\n",
    "JOIN types ON tokens.word_type_ind=types.ind\n",
    "JOIN word_loc ON word_loc.ind=types.word_loc\n",
    "JOIN pos ON pos.ind=types.pos_type\n",
    "WHERE doc_ind<=8;\"\"\"\n",
    "query_words = crsr.execute(query).fetchall()\n",
    "sentences_list = []\n",
    "tags_list = []\n",
    "for k, g in groupby(query_words, lambda x: (x[0], x[1])):\n",
    "    sent = list(g)\n",
    "    sentences_list.append([(w[2],) for w in sent])\n",
    "    tags_list.append([(tag_indices['-'.join(w[3:])],) for w in sent])\n",
    "    \n",
    "# print the second sentence as a word type index sequence\n",
    "print(sentences_list[1])\n",
    "# print the sentence as a word type sequence, using index_words dataframe from above\n",
    "print(list(index_words.at[word[0], 'Word_Type'] for word in sentences_list[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling padding and out-of-vocabulary items.\n",
    "\n",
    "The Keras model we will use below requires each element in the training input to have the same number of tokens. This means that we will need to pad every sentence that is not as long as the longest sentence in the training set. We can achieve this by adding a `0` index value to our `index_words` dataframe.\n",
    "\n",
    "Likewise, in testing and production we will inevitably run into items that are not in the vocabulary used in training the model. This can be handled by adding a row in the `index_words` dataframe with an index value beyond the current maximum for the value \"out of vocabulary.\" This ensures compatibility with the existing database values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Word_Type\n",
      "Index                  \n",
      "951    electromyography\n",
      "952                 emg\n",
      "953                 tom\n",
      "0                  $PAD\n",
      "954                $OUT\n"
     ]
    }
   ],
   "source": [
    "index_words.loc[0] = ['$PAD']\n",
    "index_words.loc[index_words.index.max() + 1] = ['$OUT']\n",
    "print(index_words.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and testing.\n",
    "\n",
    "Here, we split the data into training and testing components using `sklearn.model_selection.train_test_split`. `train_test_split` splits the sentences randomly, so the training and testing portions will both contain portions of all eight documents. This means that the testing portion of the data will provide a clear indication as to whether training the model below has been successful, but we will still need to test it again later on a fully unseen document. Here, we split the data based on a common threshold: 20% of the sentences for testing and 80% for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sentences_list, tags_list, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing X_test terms.\n",
    "\n",
    "Because we will train the model below on the `X_train` set created above, the word type numerical values found in `X_test` that are not found in `X_train` will create trouble for the model, as the values will produce word embeddings for which the model has not been trained to process. We handle this here by replacing these numerical values with the out-of-vocabulary value, which is equal to `index_words.index.max()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words:  ['*', 'qees', 'tus', 'neeg', 'uas', 'muaj', 'hom', 'kab', 'mob', 'tb', 'no', 'yuav', 'kis', 'tau', 'rau', 'lwm', 'leej', 'lwm', 'tus', '.']\n",
      "Before out-of-vocabulary conversion:  [(539,), (787,), (383,), (29,), (80,), (23,), (253,), (19,), (20,), (782,), (32,), (69,), (70,), (71,), (26,), (149,), (427,), (788,), (383,), (22,)]\n",
      "After out-of-vocabulary conversion:   [(539,), (954,), (383,), (29,), (80,), (23,), (253,), (19,), (20,), (782,), (32,), (69,), (70,), (71,), (26,), (149,), (427,), (954,), (383,), (22,)]\n"
     ]
    }
   ],
   "source": [
    "words = set(word_value for sent in X_train for word_value in sent)\n",
    "\n",
    "pre_sample_sentence_index = 10\n",
    "X_test_pre_sample = X_test[pre_sample_sentence_index]\n",
    "X_test = [[word_value if word_value in words else (index_words.index.max(),) for word_value in sent] \\\n",
    "          for sent in X_test]\n",
    "\n",
    "print('Original words: ', list(index_words.at[ind[0], 'Word_Type'] for ind in X_test_pre_sample))\n",
    "print('Before out-of-vocabulary conversion: ', X_test_pre_sample)\n",
    "print('After out-of-vocabulary conversion:  ', X_test[pre_sample_sentence_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding sentences.\n",
    "\n",
    "Next, we need to pad the sentences such that each sentence has the same length. We do this by finding the longest sentence by tokens in `X_train` and using this as the `maxlen` parameter of `keras.preprocessing.sequence.pad_sequences` for each of the four data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_MAX = len(max(X_train, key=len))\n",
    "\n",
    "X_train = pad_sequences([[w[0] for w in line] for line in X_train], maxlen=LEN_MAX, padding='post')\n",
    "y_train = pad_sequences(y_train, maxlen=LEN_MAX, padding='post')\n",
    "\n",
    "X_test = pad_sequences([[w[0] for w in line] for line in X_test], maxlen=LEN_MAX, padding='post')\n",
    "y_test = pad_sequences(y_test, maxlen=LEN_MAX, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the pretrained word embedding model.\n",
    "\n",
    "Now, we can load the Word2Vec word embedding model pretrained on the SCH corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load('word2vec_Hmong_SCH.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Populate embedding matrix.\n",
    "\n",
    "The embedding matrix in our Keras model below will use the word embedding vectors from the Word2Vec model above. However, we want to populate our embedding matrix using only those vectors that correspond to our training set. We create a matrix that can contain the full number of word indices in the database vocabulary, plus padding and out-of-vocabulary values. We then populate the matrix with the word embeddings at row positions corresponding to the word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_vocab_size = index_words.index.max() + 1\n",
    "embedding_matrix = np.zeros((maximum_vocab_size, 150))\n",
    "for ind in words:\n",
    "    try:\n",
    "        embedding_vector = word2vec_model.wv[index_words.at[ind[0], 'Word_Type']]\n",
    "    except KeyError as e:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[ind[0]] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keras model.\n",
    "\n",
    "Now, we create the Keras model. We use the Sequential() model type, as this is a sequential labeling task.\n",
    "\n",
    "We use the `weights` parameter of `Embedding` to input the word embedding matrix we just created above.\n",
    "\n",
    "We then compile the model using categorical cross-entropy as a loss, and Adam as an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nathan/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 93, 150)           143250    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 93, 512)           833536    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 93, 32)            16416     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 93, 32)            0         \n",
      "=================================================================\n",
      "Total params: 993,202\n",
      "Trainable params: 849,952\n",
      "Non-trainable params: 143,250\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(LEN_MAX, )))\n",
    "model.add(Embedding(maximum_vocab_size, 150, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag_indices))))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model.\n",
    "\n",
    "Now we train the model using the X_train data, with y_train converted to one-hot vectors using `keras.utils.np_utils.to_categorical`. We choose a batch size of 16, and set aside 20% of our training set for validation, leaving the rest for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nathan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/nathan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 224 samples, validate on 57 samples\n",
      "Epoch 1/50\n",
      "224/224 [==============================] - 8s 36ms/step - loss: 1.7080 - acc: 0.8271 - val_loss: 0.3139 - val_acc: 0.9276\n",
      "Epoch 2/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.2060 - acc: 0.9504 - val_loss: 0.1895 - val_acc: 0.9542\n",
      "Epoch 3/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.1208 - acc: 0.9716 - val_loss: 0.1416 - val_acc: 0.9662\n",
      "Epoch 4/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0818 - acc: 0.9807 - val_loss: 0.1182 - val_acc: 0.9734\n",
      "Epoch 5/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0588 - acc: 0.9872 - val_loss: 0.1031 - val_acc: 0.9764\n",
      "Epoch 6/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0438 - acc: 0.9911 - val_loss: 0.0951 - val_acc: 0.9776\n",
      "Epoch 7/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0329 - acc: 0.9935 - val_loss: 0.0900 - val_acc: 0.9785\n",
      "Epoch 8/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0258 - acc: 0.9954 - val_loss: 0.0856 - val_acc: 0.9800\n",
      "Epoch 9/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0198 - acc: 0.9970 - val_loss: 0.0836 - val_acc: 0.9802\n",
      "Epoch 10/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0154 - acc: 0.9979 - val_loss: 0.0819 - val_acc: 0.9806\n",
      "Epoch 11/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0123 - acc: 0.9986 - val_loss: 0.0811 - val_acc: 0.9808\n",
      "Epoch 12/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0099 - acc: 0.9989 - val_loss: 0.0802 - val_acc: 0.9813\n",
      "Epoch 13/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0082 - acc: 0.9993 - val_loss: 0.0802 - val_acc: 0.9806\n",
      "Epoch 14/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0069 - acc: 0.9996 - val_loss: 0.0796 - val_acc: 0.9815\n",
      "Epoch 15/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0058 - acc: 0.9997 - val_loss: 0.0804 - val_acc: 0.9809\n",
      "Epoch 16/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0050 - acc: 0.9999 - val_loss: 0.0809 - val_acc: 0.9811\n",
      "Epoch 17/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0045 - acc: 0.9998 - val_loss: 0.0800 - val_acc: 0.9813\n",
      "Epoch 18/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0038 - acc: 0.9999 - val_loss: 0.0805 - val_acc: 0.9815\n",
      "Epoch 19/50\n",
      "224/224 [==============================] - 6s 25ms/step - loss: 0.0034 - acc: 0.9999 - val_loss: 0.0807 - val_acc: 0.9815\n",
      "Epoch 20/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0030 - acc: 0.9999 - val_loss: 0.0813 - val_acc: 0.9817\n",
      "Epoch 21/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0811 - val_acc: 0.9819\n",
      "Epoch 22/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0821 - val_acc: 0.9815\n",
      "Epoch 23/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0816 - val_acc: 0.9815\n",
      "Epoch 24/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0820 - val_acc: 0.9823\n",
      "Epoch 25/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0824 - val_acc: 0.9821\n",
      "Epoch 26/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0825 - val_acc: 0.9823\n",
      "Epoch 27/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0828 - val_acc: 0.9823\n",
      "Epoch 28/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0832 - val_acc: 0.9821\n",
      "Epoch 29/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0833 - val_acc: 0.9823\n",
      "Epoch 30/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0840 - val_acc: 0.9825\n",
      "Epoch 31/50\n",
      "224/224 [==============================] - 6s 25ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0840 - val_acc: 0.9825\n",
      "Epoch 32/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0843 - val_acc: 0.9823\n",
      "Epoch 33/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0846 - val_acc: 0.9823\n",
      "Epoch 34/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 9.9564e-04 - acc: 1.0000 - val_loss: 0.0847 - val_acc: 0.9823\n",
      "Epoch 35/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 9.3773e-04 - acc: 1.0000 - val_loss: 0.0850 - val_acc: 0.9823\n",
      "Epoch 36/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 8.8720e-04 - acc: 1.0000 - val_loss: 0.0852 - val_acc: 0.9823\n",
      "Epoch 37/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 8.3691e-04 - acc: 1.0000 - val_loss: 0.0856 - val_acc: 0.9825\n",
      "Epoch 38/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 7.9975e-04 - acc: 1.0000 - val_loss: 0.0858 - val_acc: 0.9825\n",
      "Epoch 39/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 7.6513e-04 - acc: 1.0000 - val_loss: 0.0860 - val_acc: 0.9823\n",
      "Epoch 40/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 7.1958e-04 - acc: 1.0000 - val_loss: 0.0860 - val_acc: 0.9825\n",
      "Epoch 41/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 6.8776e-04 - acc: 1.0000 - val_loss: 0.0866 - val_acc: 0.9825\n",
      "Epoch 42/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 6.5721e-04 - acc: 1.0000 - val_loss: 0.0866 - val_acc: 0.9823\n",
      "Epoch 43/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 6.2592e-04 - acc: 1.0000 - val_loss: 0.0871 - val_acc: 0.9825\n",
      "Epoch 44/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 5.9832e-04 - acc: 1.0000 - val_loss: 0.0870 - val_acc: 0.9823\n",
      "Epoch 45/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 5.7293e-04 - acc: 1.0000 - val_loss: 0.0873 - val_acc: 0.9826\n",
      "Epoch 46/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 5.4859e-04 - acc: 1.0000 - val_loss: 0.0874 - val_acc: 0.9826\n",
      "Epoch 47/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 5.2936e-04 - acc: 1.0000 - val_loss: 0.0879 - val_acc: 0.9825\n",
      "Epoch 48/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 5.0700e-04 - acc: 1.0000 - val_loss: 0.0878 - val_acc: 0.9826\n",
      "Epoch 49/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 4.8735e-04 - acc: 1.0000 - val_loss: 0.0883 - val_acc: 0.9826\n",
      "Epoch 50/50\n",
      "224/224 [==============================] - 5s 24ms/step - loss: 4.7023e-04 - acc: 1.0000 - val_loss: 0.0885 - val_acc: 0.9826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc20bca128>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, to_categorical(y_train, num_classes=max(tag_indices.values()) + 1), batch_size=16, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model on test set.\n",
    "\n",
    "Now we use `evaluate` to evaluate the accuracy of the model on the test set. As mentioned above, the test set contains sentences from the same documents as the training set, so the results will be higher than on previously unseen documents, which we address below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 5ms/step\n",
      "Accuracy: 96.68332581788721 percent\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, to_categorical(y_test, num_classes=max(tag_indices.values()) + 1))\n",
    "print(\"Accuracy: {result} percent\".format(result=(scores[1]*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on unseen data.\n",
    "\n",
    "Finally, we evaluate our model on unseen data—a word position/POS-tagged ninth document, which we load from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 5ms/step\n",
      "Accuracy: 96.25993371009827 percent\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"SELECT doc_ind, sent_ind, word_type_ind, loc, pos_label FROM tokens\n",
    "JOIN types ON tokens.word_type_ind=types.ind\n",
    "JOIN word_loc ON word_loc.ind=types.word_loc\n",
    "JOIN pos ON pos.ind=types.pos_type\n",
    "WHERE doc_ind==9;\"\"\"\n",
    "query_words = crsr.execute(query).fetchall()\n",
    "sentences_list = []\n",
    "tags_list = []\n",
    "for k, g in groupby(query_words, lambda x: (x[0], x[1])):\n",
    "    sent = list(g)\n",
    "    sentences_list.append([(w[2],) for w in sent])\n",
    "    tags_list.append([(tag_indices['-'.join(w[3:])],) for w in sent])\n",
    "\n",
    "X_new = [[word_value if word_value in words else (index_words.index.max(),) for word_value in sent] \\\n",
    "          for sent in sentences_list]\n",
    "    \n",
    "X_new = pad_sequences([[w[0] for w in line] for line in X_new], maxlen=LEN_MAX, padding='post')\n",
    "y_new = pad_sequences(tags_list, maxlen=LEN_MAX, padding='post')\n",
    "\n",
    "scores = model.evaluate(X_new, to_categorical(y_new, num_classes=max(tag_indices.values()) + 1))\n",
    "print(\"Accuracy: {result} percent\".format(result=(scores[1]*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, even on an unseen text, the accuracy of this model still reaches 96.26% in this case, with an input of only about 6000 tagged words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "Altogether, combining word tokenization and POS tagging successfully tackles the problem of syllable-spacing in Hmong, and using a BiLSTM model with pretrained word embeddings using Word2Vec overcomes the limitations on available tagged data.\n",
    "\n",
    "#### References and further reading\n",
    "\n",
    "[1] Lewis, William D. and Phong Yang. 2012. Building MT for a Severely Under-Resourced Language: White Hmong. In _Proceedings of the Tenth Biennial Conference of the Association for Machine Translation in the Americas_. https://pdfs.semanticscholar.org/098c/96c2ad281ac617fbe0766623834eb295ec2c.pdf\n",
    "\n",
    "[2] Takahashi, Kanji and Kazuhide Yamamoto. 2016. Fundamental tools and resource are available for Vietnamese analysis. In _Proceedings of the 2016 International Conference on Asian Lanuage Processing_, p. 246–249. https://ieeexplore.ieee.org/document/7875978\n",
    "\n",
    "[3] Nguyen Dat Quoc, Thanh Vu, Dai Quoc Nguyen, Mark Dras and Mark Johnson. 2017. In _Proceedings of the Australasian Language Technology Association Workshop_, p. 108–113. https://www.aclweb.org/anthology/U17-1013/\n",
    "\n",
    "##### Other further reading links:\n",
    "\n",
    "Some additional inspiration for my implementation of the approach using BiLSTM above, including especially the Keras model design, can be found at https://nlpforhackers.io/lstm-pos-tagger-keras/.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
